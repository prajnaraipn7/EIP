{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eip3_Assignment2A",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajnaraipn7/EIP/blob/master/Eip3_Assignment2A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "on7uFZsjthz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kI1a5uV5xpx_"
      },
      "cell_type": "markdown",
      "source": [
        "#<font color =\"maroon\">Backpropagation Step by Step </font>\n",
        "\n",
        "@prajnaraipn@gmail.com, Batch4\n",
        "                      \n",
        "If you are building your own neural network, you will definitely need to understand how to train it. Backpropagation is a commonly used technique for training neural network. There are many resources explaining the technique, but this post will explain backpropagation with concrete example in a very detailed colorful steps.\n",
        "\n",
        "![alt text](https://hmkcode.github.io/images/ai/backpropagation.png)\n",
        "\n",
        "##Overview\n",
        "\n",
        "In this post, we will build a neural network with three layers:\n",
        "*   Input layer with two inputs neurons\n",
        "*   One hidden layer with two neurons\n",
        "*  Output layer with a single neuron\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/nn1.png)\n",
        "\n",
        "\n",
        "#Weights, weights, weights\n",
        "Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.Then, backpropagation is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
        "\n",
        "Our initial weights will be as following: \n",
        "\n",
        "<font color='blue'>w1 = 0.20, w2 = 0.02, w3 = 0.16, w4 = 0.10, w5 = 0.17 and w6 = 0.12 </font>\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fih.JPG?raw=true)\n",
        "\n",
        "##Dataset\n",
        "Our dataset has one sample with two inputs and one output.\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_dataset.png)\n",
        "\n",
        "Our single sample is as following inputs=[2, 3] and output=[1].\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_sample.png)\n",
        "\n",
        "\n",
        "##Forward Pass\n",
        "We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fih3.JPG?raw=true)\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2&3\\\\\n",
        "\\end{bmatrix}.\\begin{bmatrix}\n",
        ".20&.16\\\\\n",
        ".02&.10\\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        ".46&.62\\\\\n",
        "\\end{bmatrix}.\\begin{bmatrix}\n",
        ".17\\\\\n",
        ".12\\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        ".15\\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "##Calculating Error\n",
        "Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or <b>prediction</b>, is not even close to <b>actual output</b>. We can calculate the difference or the error as following.\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fih4.JPG?raw=true)\n",
        "\n",
        "##Reducing Error\n",
        "Our main goal of the training is to reduce the<b> error</b> or the difference between<b> prediction</b> and <b>actual output</b>. Since <b>actual output</b> is constant, “not changing”, the only way to reduce the error is to change <b>prediction</b> value. The question now is, how to change <b>prediction</b> value?\n",
        "\n",
        "By decomposing <b>prediction</b> into its basic elements we can find that <b>weights</b> are the variable elements affecting <b>prediction</b> value. In other words, in order to change <b>prediction</b> value, we need to change <b>weights</b> values.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_prediction_elements.png)\n",
        "\n",
        "\n",
        "The question now is how to <b>change\\update the weights value so that the error is reduced?</b>\n",
        "The answer is <b>Backpropagation!</b>\n",
        "\n",
        "##Backpropagation\n",
        "<b>Backpropagation</b>, short for “backward propagation of errors”, is a mechanism used to update the <b>weights</b> using<font color=\"blue\"> gradient descent</font>. It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n",
        "\n",
        "> <b>Gradient descent</b> is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n",
        "\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_update_formula.png)\n",
        "\n",
        "For example, to update <font color =\"blue\">w6</font>, we take the current <font color =\"blue\">w6</font>, and subtract the partial derivative of <b>error</b> function with respect to w6. Optionally, we multiply the derivative of the error function by a selected number to make sure that the new updated <b>weight</b> is minimizing the error function; this number is called <b>learning rate</b>.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_w6_update.png)\n",
        "\n",
        "The derivation of the error function is evaluated by applying the chain rule as following\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w6.png)\n",
        "\n",
        "So to update <font color=\"blue\">w6</font> we can apply the following formula\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_w6_update_closed_form.png)\n",
        "\n",
        "Similarly, we can derive the update formula for <font color=\"blue\">w5</font> and any other weights existing between the output and the hidden layer.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_w5_update_closed_form.png)\n",
        "\n",
        "However, when moving backward to update <font color=\"blue\">w1, w2, w3</font> and <font color=\"blue\">w4 </font>existing between input and hidden layer, the partial derivative for the error function with respect to<font colr=\"blue\"> w1</font>, for example, will be as following.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w1.png)\n",
        "\n",
        "We can find the update formula for the remaining weights <font colr=\"blue\">w2, w3 </font>and <font color=\"blue\">w4</font>  in the same way.\n",
        "\n",
        "In summary, the update formulas for all weights will be as following:\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_update_all_weights.png)\n",
        "\n",
        "We can rewrite the update formulas in matrices as following\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_update_all_weights_matrix.png)\n",
        "\n",
        "##Backward Pass\n",
        "\n",
        "Using derived formulas we can find the new <b>weights</b>.\n",
        "\n",
        "><b> Learning rate</b>: is a hyperparameter which means that we need to manually guess its value.\n",
        "\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fi.JPG?raw=true)\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w5\\\\\n",
        "w6\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".17\\\\\n",
        ".12\\\\\n",
        "\\end{bmatrix}\n",
        "-0.05(-0.85)\n",
        "\\begin{bmatrix}\n",
        ".46\\\\\n",
        ".62\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".17\\\\\n",
        ".12\\\\\n",
        "\\end{bmatrix}\n",
        "-\n",
        "\\begin{bmatrix}\n",
        "-0.019\\\\\n",
        "-0.026\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.19\\\\\n",
        "0.15\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w1&w3\\\\\n",
        "w2&w4\\\\\n",
        "\\end{bmatrix} \n",
        "= \n",
        "\\begin{bmatrix}\n",
        ".20&.16\\\\\n",
        ".02&.10\\\\\n",
        "\\end{bmatrix} \n",
        "-0.05(-0.85)\n",
        "\\begin{bmatrix}\n",
        "2\\\\\n",
        "3\\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        ".17&.12\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".20&.16\\\\\n",
        ".02&.10\\\\\n",
        "\\end{bmatrix} \n",
        "-\n",
        "\\begin{bmatrix}\n",
        "-0.014&-0.0102\\\\\n",
        "-0.0216&-0.015\\\\\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.214&0.17\\\\\n",
        "0.0416&0.115\\\\\n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "Now, using the new <b>weights</b> we will repeat the <b>forward passed</b>\n",
        "\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/gi.JPG?raw=true)\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2&3\\\\\n",
        "\\end{bmatrix}\n",
        ".\\begin{bmatrix}\n",
        "0.214&0.17\\\\\n",
        "0.0416&0.115\\\\\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.55&0.68\\\\\n",
        "\\end{bmatrix}\n",
        ".\\begin{bmatrix}\n",
        "0.19\\\\\n",
        "0.15\\\\\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".20\\\\\n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "\n",
        "We can notice that the <b>prediction</b> <font color=\"blue\">0.20 </font>is a little bit closer to <b>actual output</b> than the previously predicted one <font color=\"blue\">0.15</font>. We can repeat the same process of backward and forward pass until error is close or equal to zero.\n",
        "\n"
      ]
    }
  ]
}