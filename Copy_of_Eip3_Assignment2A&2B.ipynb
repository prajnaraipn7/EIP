{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Eip3_Assignment2A&2B",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajnaraipn7/EIP/blob/master/Copy_of_Eip3_Assignment2A%262B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QsrnCixdzQgQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2A Assignment"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kI1a5uV5xpx_"
      },
      "cell_type": "markdown",
      "source": [
        "#<font color =\"maroon\">Backpropagation Step by Step </font>\n",
        "\n",
        "@prajnaraipn@gmail.com, Batch4\n",
        "                      \n",
        "If you are building your own neural network, you will definitely need to understand how to train it. Backpropagation is a commonly used technique for training neural network. There are many resources explaining the technique, but this post will explain backpropagation with concrete example in a very detailed colorful steps.\n",
        "\n",
        "![alt text](https://hmkcode.github.io/images/ai/backpropagation.png)\n",
        "\n",
        "##Overview\n",
        "\n",
        "In this post, we will build a neural network with three layers:\n",
        "*   Input layer with two inputs neurons\n",
        "*   One hidden layer with two neurons\n",
        "*  Output layer with a single neuron\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/nn1.png)\n",
        "\n",
        "\n",
        "#Weights, weights, weights\n",
        "Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.Then, backpropagation is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n",
        "\n",
        "Our initial weights will be as following: \n",
        "\n",
        "<font color='blue'>w1 = 0.20, w2 = 0.02, w3 = 0.16, w4 = 0.10, w5 = 0.17 and w6 = 0.12 </font>\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fih.JPG?raw=true)\n",
        "\n",
        "##Dataset\n",
        "Our dataset has one sample with two inputs and one output.\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_dataset.png)\n",
        "\n",
        "Our single sample is as following inputs=[2, 3] and output=[1].\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_sample.png)\n",
        "\n",
        "\n",
        "##Forward Pass\n",
        "We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fih3.JPG?raw=true)\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2&3\\\\\n",
        "\\end{bmatrix}.\\begin{bmatrix}\n",
        ".20&.16\\\\\n",
        ".02&.10\\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        ".46&.62\\\\\n",
        "\\end{bmatrix}.\\begin{bmatrix}\n",
        ".17\\\\\n",
        ".12\\\\\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        ".15\\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "##Calculating Error\n",
        "Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or <b>prediction</b>, is not even close to <b>actual output</b>. We can calculate the difference or the error as following.\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fih4.JPG?raw=true)\n",
        "\n",
        "##Reducing Error\n",
        "Our main goal of the training is to reduce the<b> error</b> or the difference between<b> prediction</b> and <b>actual output</b>. Since <b>actual output</b> is constant, “not changing”, the only way to reduce the error is to change <b>prediction</b> value. The question now is, how to change <b>prediction</b> value?\n",
        "\n",
        "By decomposing <b>prediction</b> into its basic elements we can find that <b>weights</b> are the variable elements affecting <b>prediction</b> value. In other words, in order to change <b>prediction</b> value, we need to change <b>weights</b> values.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_prediction_elements.png)\n",
        "\n",
        "\n",
        "The question now is how to <b>change\\update the weights value so that the error is reduced?</b>\n",
        "The answer is <b>Backpropagation!</b>\n",
        "\n",
        "##Backpropagation\n",
        "<b>Backpropagation</b>, short for “backward propagation of errors”, is a mechanism used to update the <b>weights</b> using<font color=\"blue\"> gradient descent</font>. It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n",
        "\n",
        "> <b>Gradient descent</b> is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n",
        "\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_update_formula.png)\n",
        "\n",
        "For example, to update <font color =\"blue\">w6</font>, we take the current <font color =\"blue\">w6</font>, and subtract the partial derivative of <b>error</b> function with respect to w6. Optionally, we multiply the derivative of the error function by a selected number to make sure that the new updated <b>weight</b> is minimizing the error function; this number is called <b>learning rate</b>.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_w6_update.png)\n",
        "\n",
        "The derivation of the error function is evaluated by applying the chain rule as following\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w6.png)\n",
        "\n",
        "So to update <font color=\"blue\">w6</font> we can apply the following formula\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_w6_update_closed_form.png)\n",
        "\n",
        "Similarly, we can derive the update formula for <font color=\"blue\">w5</font> and any other weights existing between the output and the hidden layer.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_w5_update_closed_form.png)\n",
        "\n",
        "However, when moving backward to update <font color=\"blue\">w1, w2, w3</font> and <font color=\"blue\">w4 </font>existing between input and hidden layer, the partial derivative for the error function with respect to<font colr=\"blue\"> w1</font>, for example, will be as following.\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w1.png)\n",
        "\n",
        "We can find the update formula for the remaining weights <font colr=\"blue\">w2, w3 </font>and <font color=\"blue\">w4</font>  in the same way.\n",
        "\n",
        "In summary, the update formulas for all weights will be as following:\n",
        "\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_update_all_weights.png)\n",
        "\n",
        "We can rewrite the update formulas in matrices as following\n",
        "![alt text](http://hmkcode.github.io/images/ai/bp_update_all_weights_matrix.png)\n",
        "\n",
        "##Backward Pass\n",
        "\n",
        "Using derived formulas we can find the new <b>weights</b>.\n",
        "\n",
        "><b> Learning rate</b>: is a hyperparameter which means that we need to manually guess its value.\n",
        "\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/fi.JPG?raw=true)\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w5\\\\\n",
        "w6\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".17\\\\\n",
        ".12\\\\\n",
        "\\end{bmatrix}\n",
        "-0.05(-0.85)\n",
        "\\begin{bmatrix}\n",
        ".46\\\\\n",
        ".62\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".17\\\\\n",
        ".12\\\\\n",
        "\\end{bmatrix}\n",
        "-\n",
        "\\begin{bmatrix}\n",
        "-0.019\\\\\n",
        "-0.026\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.19\\\\\n",
        "0.15\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w1&w3\\\\\n",
        "w2&w4\\\\\n",
        "\\end{bmatrix} \n",
        "= \n",
        "\\begin{bmatrix}\n",
        ".20&.16\\\\\n",
        ".02&.10\\\\\n",
        "\\end{bmatrix} \n",
        "-0.05(-0.85)\n",
        "\\begin{bmatrix}\n",
        "2\\\\\n",
        "3\\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        ".17&.12\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".20&.16\\\\\n",
        ".02&.10\\\\\n",
        "\\end{bmatrix} \n",
        "-\n",
        "\\begin{bmatrix}\n",
        "-0.014&-0.0102\\\\\n",
        "-0.0216&-0.015\\\\\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.214&0.17\\\\\n",
        "0.0416&0.115\\\\\n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "Now, using the new <b>weights</b> we will repeat the <b>forward passed</b>\n",
        "\n",
        "![alt text](https://github.com/prajnaraipn7/img/blob/master/gi.JPG?raw=true)\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "2&3\\\\\n",
        "\\end{bmatrix}\n",
        ".\\begin{bmatrix}\n",
        "0.214&0.17\\\\\n",
        "0.0416&0.115\\\\\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.55&0.68\\\\\n",
        "\\end{bmatrix}\n",
        ".\\begin{bmatrix}\n",
        "0.19\\\\\n",
        "0.15\\\\\n",
        "\\end{bmatrix} \n",
        "=\n",
        "\\begin{bmatrix}\n",
        ".20\\\\\n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "\n",
        "We can notice that the <b>prediction</b> <font color=\"blue\">0.20 </font>is a little bit closer to <b>actual output</b> than the previously predicted one <font color=\"blue\">0.15</font>. We can repeat the same process of backward and forward pass until error is close or equal to zero.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5StF8NruzIIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2B Assignment"
      ]
    },
    {
      "metadata": {
        "id": "rcDyLljzQvZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#forward pass with weights w1=.20,w2=.02,w3=.16,w4=.10,w5=.17,w6=.12\n",
        "import numpy as np\n",
        "inp=np.array([[2,3]])\n",
        "wbh = np.array([[0.20,.16],[.02,.10]])\n",
        "hid=np.dot(inp,wbh)\n",
        "wah=np.array([[0.17],[0.12]])\n",
        "out=np.dot(hid,wah)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_V3W_CcwN-L",
        "colab_type": "code",
        "outputId": "98c68bb1-666e-45bb-f86a-a45d12e88497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1526]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "xHakylZcRaId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Calcualating the error\n",
        "Actual =1\n",
        "Error=(1/2)*(out[0][0]-Actual)**2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHU7BDBPSTap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Assuming the learning rate as 0.05, find the delta\n",
        "lr=0.05\n",
        "Delta=out[0][0]-Actual\n",
        "ld= lr*Delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TY29YJa4SWFP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#finding the better weights to further reduce the error \n",
        "bw56=wah-(np.transpose(hid)*ld)\n",
        "bw1234=wbh-(np.dot(np.transpose(inp)*ld,np.transpose(wah)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L-Gt5_yYl-vI",
        "colab_type": "code",
        "outputId": "9889aa1e-30cb-4a4a-fff2-b9db1bb4602b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "bw56"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1894902],\n",
              "       [0.1462694]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "FJfElNYRxreN",
        "colab_type": "code",
        "outputId": "dbc6b191-c07c-413c-c597-c9f424510ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "bw1234"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.2144058, 0.1701688],\n",
              "       [0.0416087, 0.1152532]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "aenwZWTAy6qa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Performing forward pass with the new weights -fiding new output\n",
        "nwhid=np.dot(inp,bw1234)\n",
        "nout=np.dot(nwhid,bw56)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32w71JVCz_HA",
        "colab_type": "code",
        "outputId": "d696d1fb-12ba-47ec-fc1a-9d8564eafea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "nout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.20526394]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "CI9gxq1kNnqm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}